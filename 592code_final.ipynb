{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "592code_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-7_szfddFJss",
        "DuK1LOBvYNir"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaoyingpan/592final_project_2021fall/blob/main/592code_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Please run this notebook on colab\n",
        "After uploading all files, please run all cells\n",
        "\n",
        "There are 2 outputs.\n",
        "\n",
        "/content/original_output.mp4 is the video generated by original model.\n",
        "\n",
        "/content/our_output.mp4 is the video generated by our model."
      ],
      "metadata": {
        "id": "enhm2YgOpvXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload files\n",
        "The paths should be like:\n",
        "\n",
        "/content/pic1.jpg\n",
        "\n",
        "/content/pic2.jpg\n",
        "\n",
        "/content/2.mp4\n",
        "\n",
        "/content/pzy1.txt\n",
        "\n",
        "/content/pzy2.txt\n",
        "\n",
        "/content/checkpoints.zip\n"
      ],
      "metadata": {
        "id": "me5NOd4tackN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All files should be uploaded under /content/.\n",
        "Please upload 2 images: pic1.jpg, pic2.jpg. They are 2 source images. They are aligned from original images, generated by preprocessing.ipynb\n",
        "1 driving video: 2.mp4. It's a driving video, aligned from orignial video. Original video is taken from Celeb-DF dataset, authorized by the owner.\n",
        "2 txt: pzy1.txt, pzy2.txt. They are landmark differences between the source images and driving video, used for weights caculation. They are generated by preprocessing.ipynb\n",
        "1 zip: checkpoints.zip. It is the pre-trained First order motion model.\n",
        "'''"
      ],
      "metadata": {
        "id": "hYS3-23oab8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python package installation and pre-trained model download"
      ],
      "metadata": {
        "id": "lVjt4mB4Yt1_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR0vZkNEkVGE",
        "outputId": "52f1467b-6316-454c-af60-d069c6a35844"
      },
      "source": [
        "!git clone https://github.com/anandpawara/Real_Time_Image_Animation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Real_Time_Image_Animation'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 163 (delta 14), reused 0 (delta 0), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (163/163), 22.12 MiB | 53.17 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHdhvDFBkXEk",
        "outputId": "63830eed-4393-422b-ae1e-3feb9f7d76c4"
      },
      "source": [
        "%cd /content/Real_Time_Image_Animation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real_Time_Image_Animation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH-R2Ph6m00r"
      },
      "source": [
        "!unzip -q /content/checkpoints.zip #The pretrained first order motion model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfUp3CikoRM",
        "outputId": "48090b8c-1194-42db-ae25-58896d5d9ff9"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting attrs==19.3.0\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting backcall==0.1.0\n",
            "  Downloading backcall-0.1.0.zip (11 kB)\n",
            "Collecting bleach==3.1.5\n",
            "  Downloading bleach-3.1.5-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting cffi==1.11.5\n",
            "  Downloading cffi-1.11.5-cp37-cp37m-manylinux1_x86_64.whl (421 kB)\n",
            "\u001b[K     |████████████████████████████████| 421 kB 36.8 MB/s \n",
            "\u001b[?25hCollecting cloudpickle==0.5.3\n",
            "  Downloading cloudpickle-0.5.3-py2.py3-none-any.whl (13 kB)\n",
            "Collecting colorama==0.4.3\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting dask==0.18.2\n",
            "  Downloading dask-0.18.2-py2.py3-none-any.whl (645 kB)\n",
            "\u001b[K     |████████████████████████████████| 645 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting decorator==4.3.0\n",
            "  Downloading decorator-4.3.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting defusedxml==0.6.0\n",
            "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.3)\n",
            "Collecting imageio==2.3.0\n",
            "  Downloading imageio-2.3.0-py2.py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 74.9 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==1.6.0\n",
            "  Downloading importlib_metadata-1.6.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ipykernel==5.2.1\n",
            "  Downloading ipykernel-5.2.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 83.9 MB/s \n",
            "\u001b[?25hCollecting ipython==7.14.0\n",
            "  Downloading ipython-7.14.0-py3-none-any.whl (782 kB)\n",
            "\u001b[K     |████████████████████████████████| 782 kB 79.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.2.0)\n",
            "Collecting jedi==0.17.0\n",
            "  Downloading jedi-0.17.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.7 MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.11.2\n",
            "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 85.4 MB/s \n",
            "\u001b[?25hCollecting jsonschema==3.2.0\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting jupyter-client==6.1.3\n",
            "  Downloading jupyter_client-6.1.3-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 81.6 MB/s \n",
            "\u001b[?25hCollecting jupyter-core==4.6.3\n",
            "  Downloading jupyter_core-4.6.3-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting kiwisolver==1.0.1\n",
            "  Downloading kiwisolver-1.0.1-cp37-cp37m-manylinux1_x86_64.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Collecting matplotlib==2.2.2\n",
            "  Downloading matplotlib-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (12.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.6 MB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (0.8.4)\n",
            "Requirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (5.6.1)\n",
            "Collecting nbformat==5.0.6\n",
            "  Downloading nbformat-5.0.6-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 87.0 MB/s \n",
            "\u001b[?25hCollecting networkx==2.1\n",
            "  Downloading networkx-2.1.zip (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 77.2 MB/s \n",
            "\u001b[?25hCollecting notebook==6.0.3\n",
            "  Downloading notebook-6.0.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 68.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.0\n",
            "  Downloading numpy-1.15.0-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 74.4 MB/s \n",
            "\u001b[?25hCollecting opencv-python==4.2.0.34\n",
            "  Downloading opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2 MB 6.6 MB/s \n",
            "\u001b[?25hCollecting packaging==20.4\n",
            "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pandas==0.23.4\n",
            "  Downloading pandas-0.23.4-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 23.2 MB/s \n",
            "\u001b[?25hCollecting pandocfilters==1.4.2\n",
            "  Downloading pandocfilters-1.4.2.tar.gz (14 kB)\n",
            "Collecting parso==0.7.0\n",
            "  Downloading parso-0.7.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (0.7.5)\n",
            "Collecting Pillow==5.2.0\n",
            "  Downloading Pillow-5.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 69.1 MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
            "Collecting prompt-toolkit==3.0.5\n",
            "  Downloading prompt_toolkit-3.0.5-py3-none-any.whl (351 kB)\n",
            "\u001b[K     |████████████████████████████████| 351 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting pycparser==2.18\n",
            "  Downloading pycparser-2.18.tar.gz (245 kB)\n",
            "\u001b[K     |████████████████████████████████| 245 kB 84.9 MB/s \n",
            "\u001b[?25hCollecting pygit==0.1\n",
            "  Downloading pygit-0.1.tar.gz (6.7 kB)\n",
            "Requirement already satisfied: Pygments==2.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 42)) (2.6.1)\n",
            "Collecting pyparsing==2.2.0\n",
            "  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.16.0\n",
            "  Downloading pyrsistent-0.16.0.tar.gz (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 83.5 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.7.3\n",
            "  Downloading python_dateutil-2.7.3-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 73.8 MB/s \n",
            "\u001b[?25hCollecting pytz==2018.5\n",
            "  Downloading pytz-2018.5-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 84.1 MB/s \n",
            "\u001b[?25hCollecting PyWavelets==0.5.2\n",
            "  Downloading PyWavelets-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 51.0 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==227 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pywin32==227\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxhmbskjovMP"
      },
      "source": [
        "# Download facial landmark detection model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Su0eBXTZMR"
      },
      "source": [
        "import dlib\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "def get_landmark(img, predictor):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # img = dlib.load_rgb_image(filepath)\n",
        "    dets = detector(img, 1)\n",
        "\n",
        "    shape = None\n",
        "    for k, d in enumerate(dets):\n",
        "        shape = predictor(img, d)\n",
        "\n",
        "    if not shape:\n",
        "        raise Exception(\"Could not find face in image! Please try another image!\")\n",
        "\n",
        "    t = list(shape.parts())\n",
        "    a = []\n",
        "    for tt in t:\n",
        "        a.append([tt.x, tt.y])\n",
        "    lm = np.array(a)\n",
        "    return lm\n",
        "\n",
        "def align_face(img, predictor, output_size=256, transform_size=256):\n",
        "    \"\"\"\n",
        "\t:param filepath: str\n",
        "\t:return: PIL Image\n",
        "\t\"\"\"\n",
        "\n",
        "    lm = get_landmark(img, predictor)\n",
        "\n",
        "    lm_chin = lm[0: 17]  # left-right\n",
        "    lm_eyebrow_left = lm[17: 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22: 27]  # left-right\n",
        "    lm_nose = lm[27: 31]  # top-down\n",
        "    lm_nostrils = lm[31: 36]  # top-down\n",
        "    lm_eye_left = lm[36: 42]  # left-clockwise\n",
        "    lm_eye_right = lm[42: 48]  # left-clockwise\n",
        "    lm_mouth_outer = lm[48: 60]  # left-clockwise\n",
        "    lm_mouth_inner = lm[60: 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left = np.mean(lm_eye_left, axis=0)\n",
        "    eye_right = np.mean(lm_eye_right, axis=0)\n",
        "    eye_avg = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye = eye_right - eye_left\n",
        "    mouth_left = lm_mouth_outer[0]\n",
        "    mouth_right = lm_mouth_outer[6]\n",
        "    mouth_avg = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # read image\n",
        "    # img = PIL.Image.open(filepath)\n",
        "    img = Image.fromarray(np.uint8(img[:,:,::-1]))\n",
        "    # img = img[:,:,::-1]\n",
        "    # img = img[...,::-1]\n",
        "\n",
        "    enable_padding = True\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "            int(np.ceil(max(quad[:, 1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n",
        "            min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "           int(np.ceil(max(quad[:, 1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n",
        "           max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n",
        "                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    # Save aligned image.\n",
        "    return img\n",
        "\n",
        "\n",
        "def run_alignment(img):\n",
        "    import dlib\n",
        "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
        "        print('Downloading files for aligning face image...')\n",
        "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        print('Done.')\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(img=img, predictor=predictor)\n",
        "    return aligned_image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download StyleGAN and StyleGAN encoder models"
      ],
      "metadata": {
        "id": "lEC4Fh4dXIIJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBpuNM6QQvAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c58933-891c-4b2a-889b-8585dc3cea29"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!git clone https://github.com/yuval-alaluf/restyle-encoder.git\n",
        "\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip -n ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "os.chdir('/content/restyle-encoder')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp\n",
        "\n",
        "def get_download_model_command():\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = os.path.join(os.path.dirname(current_directory), 'restyle-encoder', \"pretrained_models\")\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1sw6I2lRIB0MpuJkpc8F5BJiSZrc0hjfE' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1sw6I2lRIB0MpuJkpc8F5BJiSZrc0hjfE\" -O {SAVE_PATH}/restyle_psp_ffhq_encode.pt && rm -rf /tmp/cookies.txt\"\"\".format(SAVE_PATH=save_path)\n",
        "    \n",
        "    # url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url\n",
        "\n",
        "download_command = get_download_model_command()\n",
        "model_path = \"pretrained_models/restyle_psp_ffhq_encode.pt\"\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "if not os.path.exists(model_path) or os.path.getsize(model_path) < 1000000:\n",
        "    print('Downloading ReStyle model for ffhq_encoder...')\n",
        "    os.system(f\"wget {download_command}\")\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(model_path) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print('ReStyle model for ffhq_encoder already exists!')\n",
        "\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "\n",
        "\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "trans = transforms.ToTensor()\n",
        "\n",
        "\n",
        "\n",
        "def get_avg_image(net):\n",
        "    avg_image = net(net.latent_avg.unsqueeze(0),\n",
        "                    input_code=True,\n",
        "                    randomize_noise=False,\n",
        "                    return_latents=False,\n",
        "                    average_code=True)[0]\n",
        "    avg_image = avg_image.to('cuda').float().detach()\n",
        "    return avg_image\n",
        "\n",
        "opts.n_iters_per_batch = 5\n",
        "opts.resize_outputs = False  # generate outputs at full resolution\n",
        "\n",
        "from utils.inference_utils import run_on_batch\n",
        "\n",
        "\n",
        "def latent2img(net,latent_code):\n",
        "    image = net(latent_code.unsqueeze(0),\n",
        "                    input_code=True,\n",
        "                    randomize_noise=False,\n",
        "                    return_latents=False,\n",
        "                    average_code=True)[0]\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'restyle-encoder'...\n",
            "remote: Enumerating objects: 323, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 323 (delta 49), reused 52 (delta 21), pack-reused 215\u001b[K\n",
            "Receiving objects: 100% (323/323), 28.13 MiB | 61.42 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "--2021-12-16 22:39:55--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211216T223955Z&X-Amz-Expires=300&X-Amz-Signature=495ff2fb47cdac41649029f25efadd24f08f5fc959d4367c5769e02883c94fca&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-12-16 22:39:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211216T223955Z&X-Amz-Expires=300&X-Amz-Signature=495ff2fb47cdac41649029f25efadd24f08f5fc959d4367c5769e02883c94fca&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-12-16 22:39:55 (5.96 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "Downloading ReStyle model for ffhq_encoder...\n",
            "Done.\n",
            "Loading ReStyle pSp from checkpoint: pretrained_models/restyle_psp_ffhq_encode.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TyiSWfcXIUF"
      },
      "source": [
        "# Video generated by the original First Order Motion model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Real_Time_Image_Animation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTntzp-2aQJ9",
        "outputId": "638a5db7-3750-4ce2-d110-222ff954cee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real_Time_Image_Animation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B26nI3c8YLii",
        "outputId": "2fd97c84-c80f-4144-f907-0bff4cf43a38"
      },
      "source": [
        "import imageio\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from animate import normalize_kp\n",
        "from demo import load_checkpoints\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "frame_index = 0\n",
        "video_path = \"/content/2.mp4\"\n",
        "source_path = \"/content/pic2.jpg\"\n",
        "checkpoint_path = \"/content/Real_Time_Image_Animation/vox-adv-cpk.pth.tar\"\n",
        "\n",
        "video_obj = cv2.VideoCapture(video_path)\n",
        "success,image = video_obj.read()\n",
        "all_frames = [image]\n",
        "while success:\n",
        "    all_frames.append(image)\n",
        "    success,image = video_obj.read()\n",
        "    \n",
        "source_image = imageio.imread(source_path)\n",
        "source_image = cv2.flip(source_image,1)\n",
        "source_image = resize(source_image,(256,256))[..., :3]\n",
        "\n",
        "generator, kp_detector = load_checkpoints(config_path='/content/Real_Time_Image_Animation/config/vox-256.yaml', checkpoint_path=checkpoint_path)\n",
        "source_image1 = all_frames[frame_index]/255\n",
        "# source1 = cv2.flip(source1,1)\n",
        "\n",
        "source1 = torch.tensor(source_image1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "\n",
        "kp_driving_initial = kp_detector(source1)\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "    os.mkdir('output')\n",
        "\n",
        "relative=True\n",
        "adapt_movement_scale=True\n",
        "cpu=False\n",
        "\n",
        "cap = cv2.VideoCapture(video_path) \n",
        "print(\"[INFO] Loading video from the given path\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out1 = cv2.VideoWriter('/content/original_output.mp4', fourcc, 12, (256*3 , 256), True)\n",
        "\n",
        "cv2_source = cv2.cvtColor(source_image.astype('float32'),cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "\n",
        "with torch.no_grad() :\n",
        "    predictions = []\n",
        "    source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "    if not cpu:\n",
        "        source = source.cuda()\n",
        "    kp_source = kp_detector(source)\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    \n",
        "    while(True):\n",
        "        ret, frame = cap.read()\n",
        "        # frame = cv2.flip(frame,1)\n",
        "        if ret == True:\n",
        "            frame1 = resize(frame,(256,256))[..., :3]\n",
        "            \n",
        "            \n",
        "            frame_test = torch.tensor(frame1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "\n",
        "            driving_frame = frame_test\n",
        "            if not cpu:\n",
        "                driving_frame = driving_frame.cuda()\n",
        "            kp_driving = kp_detector(driving_frame)\n",
        "            kp_norm = normalize_kp(kp_source=kp_source,\n",
        "                                kp_driving=kp_driving,\n",
        "                                kp_driving_initial=kp_driving_initial, \n",
        "                                use_relative_movement=relative,\n",
        "                                use_relative_jacobian=relative, \n",
        "                                adapt_movement_scale=adapt_movement_scale)\n",
        "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
        "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            im = np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
        "            im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
        "            joinedFrame = np.concatenate((cv2_source,im,frame1),axis=1)\n",
        "            out1.write(img_as_ubyte(joinedFrame))\n",
        "            # out1.write(img_as_ubyte(im))\n",
        "            count += 1\n",
        "            if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "        \n",
        "    cap.release()\n",
        "    out1.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading video from the given path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mlh-WgcSSp"
      },
      "source": [
        "#Video generated by our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7_szfddFJss"
      },
      "source": [
        "## Generate weights for 2 source images\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuEcPCqY9zNl"
      },
      "source": [
        "def loss2weight(txt_path1, txt_path2):\n",
        "\n",
        "  loss1 = np.loadtxt(txt_path1)\n",
        "  loss2 = np.loadtxt(txt_path2)\n",
        "  weight = np.zeros((2,len(loss1)))\n",
        "  for i in range(weight.shape[1]):\n",
        "    # print(i)\n",
        "    # print(weight)\n",
        "    weight[0,i] = np.exp(-loss1[i])/(np.exp(-loss1[i])+ np.exp(-loss2[i]))\n",
        "    weight[1,i] = 1 - weight[0,i]\n",
        "\n",
        "  return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNPGF-XeotbC"
      },
      "source": [
        "import numpy as np\n",
        "txt_path1 = '/content/pzy1.txt'\n",
        "txt_path2 = '/content/pzy2.txt'\n",
        "weight = loss2weight(txt_path1, txt_path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate video from 2 source images\n"
      ],
      "metadata": {
        "id": "a3CdYcCImjZD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuK1LOBvYNir"
      },
      "source": [
        "###Generate latent code for merging 2 videos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Real_Time_Image_Animation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-fGqSMWBeJx",
        "outputId": "775f101f-8816-4947-ecaf-3973fcdce87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real_Time_Image_Animation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew8uWhEtDyZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b89a5d-4f0b-4685-bb7c-088c36e82350"
      },
      "source": [
        "#run this cell, latent codes for every frames are in all_latent1\n",
        "import imageio\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from animate import normalize_kp\n",
        "from demo import load_checkpoints\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "frame_index = 00\n",
        "print(\"[INFO] loading source image and checkpoint...\")\n",
        "source_path = \"/content/pic1.jpg\"\n",
        "checkpoint_path = \"/content/Real_Time_Image_Animation/vox-adv-cpk.pth.tar\"\n",
        "\n",
        "video_path = \"/content/2.mp4\"\n",
        "video_obj = cv2.VideoCapture(video_path)\n",
        "success,image = video_obj.read()\n",
        "all_frames = [image]\n",
        "while success:\n",
        "    all_frames.append(image)   \n",
        "    success,image = video_obj.read()\n",
        "\n",
        "\n",
        "source_image = imageio.imread(source_path)\n",
        "source_image = resize(source_image,(256,256))[..., :3]\n",
        "\n",
        "generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml', checkpoint_path=checkpoint_path)\n",
        "\n",
        "source_image1 = all_frames[frame_index]/255\n",
        "source1 = torch.tensor(source_image1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "kp_driving_initial = kp_detector(source1)\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "    os.mkdir('output')\n",
        "\n",
        "\n",
        "relative=True\n",
        "adapt_movement_scale=True\n",
        "cpu=False\n",
        "\n",
        "cap = cv2.VideoCapture(video_path) \n",
        "print(\"[INFO] Loading video from the given path\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out1 = cv2.VideoWriter('output/1.mp4', fourcc, 12, (256*3 , 256), True)\n",
        "\n",
        "\n",
        "cv2_source = cv2.cvtColor(source_image.astype('float32'),cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "all_latent1 = []\n",
        "\n",
        "with torch.no_grad() :\n",
        "    predictions = []\n",
        "    source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "    if not cpu:\n",
        "        source = source.cuda()\n",
        "    kp_source = kp_detector(source)\n",
        "    count = 0\n",
        "    while(True):\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            frame1 = resize(frame,(256,256))[..., :3]\n",
        "          \n",
        "            frame_test = torch.tensor(frame1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "\n",
        "            driving_frame = frame_test\n",
        "            if not cpu:\n",
        "                driving_frame = driving_frame.cuda()\n",
        "            kp_driving = kp_detector(driving_frame)\n",
        "            kp_norm = normalize_kp(kp_source=kp_source,\n",
        "                                kp_driving=kp_driving,\n",
        "                                kp_driving_initial=kp_driving_initial, \n",
        "                                use_relative_movement=relative,\n",
        "                                use_relative_jacobian=relative, \n",
        "                                adapt_movement_scale=adapt_movement_scale)\n",
        "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
        "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            im = np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
        "            im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
        "            \n",
        "            # if count == 0:\n",
        "            img = (255*im/np.max(im)).astype(np.uint8)\n",
        "            cv2.imwrite('/content/1.jpg',img)\n",
        "            img = cv2.imread('/content/1.jpg')\n",
        "\n",
        "            input_image = run_alignment(img)\n",
        "            img_transforms = transform\n",
        "            transformed_image = img_transforms(input_image)\n",
        "            with torch.no_grad():\n",
        "              avg_image = get_avg_image(net)\n",
        "              result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), net, opts, avg_image)\n",
        "            latent_code = torch.Tensor(result_latents[0][4]).cuda()\n",
        "            all_latent1.append(latent_code)\n",
        "            joinedFrame = np.concatenate((cv2_source,im,frame1),axis=1)\n",
        "            out1.write(img_as_ubyte(joinedFrame))\n",
        "            count += 1\n",
        "            if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "        \n",
        "    cap.release()\n",
        "    out1.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading source image and checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading video from the given path\n",
            "Downloading files for aligning face image...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR9ZFDp-o_aG"
      },
      "source": [
        "####Merge latent codes with weigts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf8TrrU8o-Zv",
        "outputId": "4343c34a-466b-4f88-f959-5548ad33e150"
      },
      "source": [
        "import imageio\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from animate import normalize_kp\n",
        "from demo import load_checkpoints\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "frame_index = 82\n",
        "video_path = \"/content/2.mp4\"\n",
        "video_obj = cv2.VideoCapture(video_path)\n",
        "success,image = video_obj.read()\n",
        "count = 0\n",
        "all_frames = [image]\n",
        "while success:\n",
        "    count += 1\n",
        "    all_frames.append(image)   \n",
        "    success,image = video_obj.read()\n",
        "    \n",
        "\n",
        "print(\"[INFO] loading source image and checkpoint...\")\n",
        "source_path2 = \"/content/pic1.jpg\"\n",
        "source_path = \"/content/pic2.jpg\"\n",
        "checkpoint_path = \"/content/Real_Time_Image_Animation/vox-adv-cpk.pth.tar\"\n",
        "video_path = \"/content/2.mp4\"\n",
        "\n",
        "source_image2 = imageio.imread(source_path2)\n",
        "source_image2 = resize(source_image2,(256,256))[..., :3]\n",
        "\n",
        "source_image = imageio.imread(source_path)\n",
        "source_image = resize(source_image,(256,256))[..., :3]\n",
        "\n",
        "generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml', checkpoint_path=checkpoint_path)\n",
        "source_image1 = all_frames[frame_index]/255\n",
        "source1 = torch.tensor(source_image1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "kp_driving_initial = kp_detector(source1)\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "    os.mkdir('output')\n",
        "\n",
        "\n",
        "relative=True\n",
        "adapt_movement_scale=True\n",
        "cpu=False\n",
        "\n",
        "cap = cv2.VideoCapture(video_path) \n",
        "print(\"[INFO] Loading video from the given path\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out1 = cv2.VideoWriter('/content/our_output.mp4', fourcc, 12, (256*4 , 256), True)\n",
        "# out1 = cv2.VideoWriter('output/cell_test2.mp4', fourcc, 12, (256 , 256), True)\n",
        "\n",
        "cv2_source1 = cv2.cvtColor(source_image2.astype('float32'),cv2.COLOR_BGR2RGB)\n",
        "cv2_source = cv2.cvtColor(source_image.astype('float32'),cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            \n",
        "\n",
        "with torch.no_grad() :\n",
        "    predictions = []\n",
        "    source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "    if not cpu:\n",
        "        source = source.cuda()\n",
        "    kp_source = kp_detector(source)\n",
        "    count = 0\n",
        "\n",
        "    while(True):\n",
        "        ret, frame = cap.read()\n",
        "        # frame = cv2.flip(frame,1)\n",
        "        if ret == True:\n",
        "            frame1 = resize(frame,(256,256))[..., :3]\n",
        "               \n",
        "            frame_test = torch.tensor(frame1[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "\n",
        "            driving_frame = frame_test\n",
        "            if not cpu:\n",
        "                driving_frame = driving_frame.cuda()\n",
        "            kp_driving = kp_detector(driving_frame)\n",
        "            kp_norm = normalize_kp(kp_source=kp_source,\n",
        "                                kp_driving=kp_driving,\n",
        "                                kp_driving_initial=kp_driving_initial, \n",
        "                                use_relative_movement=relative,\n",
        "                                use_relative_jacobian=relative, \n",
        "                                adapt_movement_scale=adapt_movement_scale)\n",
        "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
        "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            im = np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
        "            im = cv2.cvtColor(im,cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # if count == 0:\n",
        "            img = (255*im/np.max(im)).astype(np.uint8)\n",
        "            cv2.imwrite('/content/11.jpg',img)\n",
        "            img = cv2.imread('/content/11.jpg')\n",
        "\n",
        "            input_image = run_alignment(img)\n",
        "            img_transforms = transform\n",
        "            transformed_image = img_transforms(input_image)\n",
        "            with torch.no_grad():\n",
        "              avg_image = get_avg_image(net)\n",
        "              result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), net, opts, avg_image)\n",
        "            \n",
        "            latent_code = torch.Tensor(result_latents[0][4]).cuda()\n",
        "\n",
        "            latent_combine = weight[0,count]*all_latent1[count] + weight[1,count]*latent_code\n",
        "\n",
        "            reconstructed_img = latent2img(net, latent_combine)\n",
        "            reconstructed_img = tensor2im(reconstructed_img)\n",
        "            reconstructed_img.save('/content/22.jpg')\n",
        "            im = cv2.imread('/content/22.jpg')\n",
        "            im = im.astype(np.float32)\n",
        "            im = im/255\n",
        "            dir = os.path.join(\"/content/frames_merged\",str(count).zfill(4)+\".jpg\")\n",
        "            cv2.imwrite(dir,(im*255))\n",
        "            joinedFrame = np.concatenate((cv2_source1,cv2_source,im,frame1),axis=1)\n",
        "            out1.write(img_as_ubyte(joinedFrame))\n",
        "            count += 1\n",
        "            if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "        \n",
        "    cap.release()\n",
        "    out1.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading source image and checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading video from the given path\n"
          ]
        }
      ]
    }
  ]
}